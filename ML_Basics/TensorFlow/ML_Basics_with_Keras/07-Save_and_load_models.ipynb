{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5d2be9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-29 08:38:22.165110: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-11-29 08:38:22.165123: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0-rc1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfde222",
   "metadata": {},
   "source": [
    "MNIST dataset for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1911d1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "399c9c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train_labels[:1000]\n",
    "test_labels = test_labels[:1000]\n",
    "\n",
    "train_images = train_images[:1000].reshape(-1, 28*28) / 255.0\n",
    "test_images = test_images[:1000].reshape(-1, 28*28) / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f26c9e4",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0beea2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_2 (Dense)             (None, 512)               401920    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        keras.layers.Dense(512, activation='relu', input_shape=(784,)),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        keras.layers.Dense(10)\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "                 loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                 metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ae4ba0",
   "metadata": {},
   "source": [
    "## Save checkpoints during training\n",
    "\n",
    " The `tf.keras.callbacks.ModelCheckpoint` callback allows to continually save the model both during and at the end of training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50628971",
   "metadata": {},
   "source": [
    "### Checkpoint callback usage\n",
    "\n",
    "Create a `tf.keras.callbacks.ModelCheckpoint` callback that saves weights only during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "438a47d3",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 1/32 [..............................] - ETA: 7s - loss: 2.3345 - sparse_categorical_accuracy: 0.1562\n",
      "Epoch 1: saving model to training_1/cp.ckpt\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 1.1513 - sparse_categorical_accuracy: 0.6740 - val_loss: 0.7679 - val_sparse_categorical_accuracy: 0.7540\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4152 - sparse_categorical_accuracy: 0.8780\n",
      "Epoch 2: saving model to training_1/cp.ckpt\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4152 - sparse_categorical_accuracy: 0.8780 - val_loss: 0.5206 - val_sparse_categorical_accuracy: 0.8390\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.2905 - sparse_categorical_accuracy: 0.9250\n",
      "Epoch 3: saving model to training_1/cp.ckpt\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.2905 - sparse_categorical_accuracy: 0.9250 - val_loss: 0.4640 - val_sparse_categorical_accuracy: 0.8560\n",
      "Epoch 4/10\n",
      " 1/32 [..............................] - ETA: 0s - loss: 0.1332 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 4: saving model to training_1/cp.ckpt\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.2082 - sparse_categorical_accuracy: 0.9490 - val_loss: 0.4292 - val_sparse_categorical_accuracy: 0.8590\n",
      "Epoch 5/10\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.1573 - sparse_categorical_accuracy: 0.9667\n",
      "Epoch 5: saving model to training_1/cp.ckpt\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1571 - sparse_categorical_accuracy: 0.9670 - val_loss: 0.4145 - val_sparse_categorical_accuracy: 0.8680\n",
      "Epoch 6/10\n",
      " 1/32 [..............................] - ETA: 0s - loss: 0.0955 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 6: saving model to training_1/cp.ckpt\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1122 - sparse_categorical_accuracy: 0.9790 - val_loss: 0.4204 - val_sparse_categorical_accuracy: 0.8630\n",
      "Epoch 7/10\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.0805 - sparse_categorical_accuracy: 0.9929\n",
      "Epoch 7: saving model to training_1/cp.ckpt\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0805 - sparse_categorical_accuracy: 0.9930 - val_loss: 0.4026 - val_sparse_categorical_accuracy: 0.8710\n",
      "Epoch 8/10\n",
      " 1/32 [..............................] - ETA: 0s - loss: 0.0858 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 8: saving model to training_1/cp.ckpt\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0609 - sparse_categorical_accuracy: 0.9950 - val_loss: 0.4162 - val_sparse_categorical_accuracy: 0.8620\n",
      "Epoch 9/10\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.0479 - sparse_categorical_accuracy: 0.9970\n",
      "Epoch 9: saving model to training_1/cp.ckpt\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0476 - sparse_categorical_accuracy: 0.9970 - val_loss: 0.4012 - val_sparse_categorical_accuracy: 0.8770\n",
      "Epoch 10/10\n",
      " 1/32 [..............................] - ETA: 0s - loss: 0.0190 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 10: saving model to training_1/cp.ckpt\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0387 - sparse_categorical_accuracy: 0.9960 - val_loss: 0.4061 - val_sparse_categorical_accuracy: 0.8740\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7ff474e390>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_path = 'training_1/cp.ckpt'\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                save_weights_only=True,\n",
    "                                                verbose=1)\n",
    "\n",
    "model.fit(train_images,\n",
    "         train_labels,\n",
    "         epochs=10,\n",
    "         validation_data=(test_images, test_labels),\n",
    "         callbacks=[cp_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea764ee9",
   "metadata": {},
   "source": [
    "This creates a single collection of TensorFlow checkpoint files that are updated at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b35c6b5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cp.ckpt.data-00000-of-00001', 'checkpoint', 'cp.ckpt.index']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8118e0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 666us/step - loss: 2.3961 - sparse_categorical_accuracy: 0.0640\n",
      "Untrained model, accuracy:  6.40%\n"
     ]
    }
   ],
   "source": [
    "# Create a basic model instance\n",
    "model = create_model()\n",
    "\n",
    "loss, acc = model.evaluate(test_images, test_labels, verbose=1)\n",
    "print('Untrained model, accuracy: {:5.2f}%'.format(100*acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b55e06d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 627us/step - loss: 0.4061 - sparse_categorical_accuracy: 0.8740\n",
      "Restores model, accuracy: 87.40%\n"
     ]
    }
   ],
   "source": [
    "# Load the weights\n",
    "model.load_weights(checkpoint_path)\n",
    "\n",
    "loss, acc = model.evaluate(test_images, test_labels, verbose=1)\n",
    "print('Restores model, accuracy: {:5.2f}%'.format(100*acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23ea4b9",
   "metadata": {},
   "source": [
    "## Checkpoint callback options\n",
    "\n",
    "The callback provides several options to provide unique names for checkpoints and adjust the checkpointing frequency.\n",
    "\n",
    "Train a new model, and save uniquely named checkpoints once every five epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f927df1",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 1.1542 - sparse_categorical_accuracy: 0.6700 - val_loss: 0.7444 - val_sparse_categorical_accuracy: 0.7710\n",
      "Epoch 2/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4357 - sparse_categorical_accuracy: 0.8700 - val_loss: 0.5345 - val_sparse_categorical_accuracy: 0.8270\n",
      "Epoch 3/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.2923 - sparse_categorical_accuracy: 0.9220 - val_loss: 0.4559 - val_sparse_categorical_accuracy: 0.8600\n",
      "Epoch 4/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.2216 - sparse_categorical_accuracy: 0.9430 - val_loss: 0.4354 - val_sparse_categorical_accuracy: 0.8680\n",
      "Epoch 5/50\n",
      " 1/32 [..............................] - ETA: 0s - loss: 0.1241 - sparse_categorical_accuracy: 0.9688\n",
      "Epoch 5: saving model to training_2/cp-0005.ckpt\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1565 - sparse_categorical_accuracy: 0.9640 - val_loss: 0.4425 - val_sparse_categorical_accuracy: 0.8600\n",
      "Epoch 6/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1131 - sparse_categorical_accuracy: 0.9800 - val_loss: 0.4472 - val_sparse_categorical_accuracy: 0.8550\n",
      "Epoch 7/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0858 - sparse_categorical_accuracy: 0.9900 - val_loss: 0.4339 - val_sparse_categorical_accuracy: 0.8620\n",
      "Epoch 8/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0636 - sparse_categorical_accuracy: 0.9940 - val_loss: 0.4094 - val_sparse_categorical_accuracy: 0.8700\n",
      "Epoch 9/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0521 - sparse_categorical_accuracy: 0.9960 - val_loss: 0.4295 - val_sparse_categorical_accuracy: 0.8640\n",
      "Epoch 10/50\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.0376 - sparse_categorical_accuracy: 0.9990\n",
      "Epoch 10: saving model to training_2/cp-0010.ckpt\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0373 - sparse_categorical_accuracy: 0.9990 - val_loss: 0.4077 - val_sparse_categorical_accuracy: 0.8740\n",
      "Epoch 11/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0298 - sparse_categorical_accuracy: 0.9990 - val_loss: 0.4192 - val_sparse_categorical_accuracy: 0.8690\n",
      "Epoch 12/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0273 - sparse_categorical_accuracy: 0.9990 - val_loss: 0.4099 - val_sparse_categorical_accuracy: 0.8690\n",
      "Epoch 13/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0217 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4218 - val_sparse_categorical_accuracy: 0.8710\n",
      "Epoch 14/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0183 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4175 - val_sparse_categorical_accuracy: 0.8720\n",
      "Epoch 15/50\n",
      "30/32 [===========================>..] - ETA: 0s - loss: 0.0152 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 15: saving model to training_2/cp-0015.ckpt\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0152 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4243 - val_sparse_categorical_accuracy: 0.8770\n",
      "Epoch 16/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0124 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4369 - val_sparse_categorical_accuracy: 0.8720\n",
      "Epoch 17/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0113 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4315 - val_sparse_categorical_accuracy: 0.8730\n",
      "Epoch 18/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0105 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4333 - val_sparse_categorical_accuracy: 0.8750\n",
      "Epoch 19/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0090 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4258 - val_sparse_categorical_accuracy: 0.8760\n",
      "Epoch 20/50\n",
      "30/32 [===========================>..] - ETA: 0s - loss: 0.0077 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 20: saving model to training_2/cp-0020.ckpt\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0078 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4370 - val_sparse_categorical_accuracy: 0.8730\n",
      "Epoch 21/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0082 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4453 - val_sparse_categorical_accuracy: 0.8730\n",
      "Epoch 22/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0070 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4486 - val_sparse_categorical_accuracy: 0.8680\n",
      "Epoch 23/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0057 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4497 - val_sparse_categorical_accuracy: 0.8720\n",
      "Epoch 24/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0050 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4512 - val_sparse_categorical_accuracy: 0.8730\n",
      "Epoch 25/50\n",
      " 1/32 [..............................] - ETA: 0s - loss: 0.0050 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 25: saving model to training_2/cp-0025.ckpt\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0045 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4557 - val_sparse_categorical_accuracy: 0.8710\n",
      "Epoch 26/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0046 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4587 - val_sparse_categorical_accuracy: 0.8710\n",
      "Epoch 27/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0040 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4481 - val_sparse_categorical_accuracy: 0.8710\n",
      "Epoch 28/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0039 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4606 - val_sparse_categorical_accuracy: 0.8750\n",
      "Epoch 29/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0033 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4642 - val_sparse_categorical_accuracy: 0.8730\n",
      "Epoch 30/50\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.0034 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 30: saving model to training_2/cp-0030.ckpt\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0034 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4656 - val_sparse_categorical_accuracy: 0.8740\n",
      "Epoch 31/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0034 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4704 - val_sparse_categorical_accuracy: 0.8730\n",
      "Epoch 32/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0029 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4727 - val_sparse_categorical_accuracy: 0.8740\n",
      "Epoch 33/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0028 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4750 - val_sparse_categorical_accuracy: 0.8720\n",
      "Epoch 34/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0026 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4707 - val_sparse_categorical_accuracy: 0.8750\n",
      "Epoch 35/50\n",
      " 1/32 [..............................] - ETA: 0s - loss: 0.0045 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 35: saving model to training_2/cp-0035.ckpt\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0022 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4746 - val_sparse_categorical_accuracy: 0.8710\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0023 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4745 - val_sparse_categorical_accuracy: 0.8750\n",
      "Epoch 37/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0018 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4789 - val_sparse_categorical_accuracy: 0.8730\n",
      "Epoch 38/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0022 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4841 - val_sparse_categorical_accuracy: 0.8720\n",
      "Epoch 39/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0020 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4837 - val_sparse_categorical_accuracy: 0.8780\n",
      "Epoch 40/50\n",
      " 1/32 [..............................] - ETA: 0s - loss: 0.0026 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 40: saving model to training_2/cp-0040.ckpt\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0021 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4961 - val_sparse_categorical_accuracy: 0.8720\n",
      "Epoch 41/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0021 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4796 - val_sparse_categorical_accuracy: 0.8790\n",
      "Epoch 42/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0018 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4853 - val_sparse_categorical_accuracy: 0.8740\n",
      "Epoch 43/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0016 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4905 - val_sparse_categorical_accuracy: 0.8750\n",
      "Epoch 44/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0015 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4891 - val_sparse_categorical_accuracy: 0.8770\n",
      "Epoch 45/50\n",
      "30/32 [===========================>..] - ETA: 0s - loss: 0.0014 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 45: saving model to training_2/cp-0045.ckpt\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0014 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4937 - val_sparse_categorical_accuracy: 0.8740\n",
      "Epoch 46/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0014 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4937 - val_sparse_categorical_accuracy: 0.8730\n",
      "Epoch 47/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0013 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4992 - val_sparse_categorical_accuracy: 0.8780\n",
      "Epoch 48/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0012 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4987 - val_sparse_categorical_accuracy: 0.8750\n",
      "Epoch 49/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0013 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.5003 - val_sparse_categorical_accuracy: 0.8730\n",
      "Epoch 50/50\n",
      "30/32 [===========================>..] - ETA: 0s - loss: 0.0012 - sparse_categorical_accuracy: 1.0000    \n",
      "Epoch 50: saving model to training_2/cp-0050.ckpt\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0012 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.5048 - val_sparse_categorical_accuracy: 0.8750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7ff00adcc0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_path = 'training_2/cp-{epoch:04d}.ckpt'\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# Create a callback that saves the model's weights every 5 epochs\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                verbose=1,\n",
    "                                                save_weights_only=True,\n",
    "                                                save_freq=5*batch_size)\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "model.save_weights(checkpoint_path.format(epoch=0))\n",
    "\n",
    "model.fit(train_images,\n",
    "         train_labels,\n",
    "         epochs=50,\n",
    "         batch_size=batch_size,\n",
    "         callbacks=[cp_callback],\n",
    "         validation_data=(test_images, test_labels),\n",
    "         verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3235cad",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cp-0040.ckpt.index',\n",
       " 'cp-0010.ckpt.data-00000-of-00001',\n",
       " 'cp-0030.ckpt.data-00000-of-00001',\n",
       " 'cp-0025.ckpt.index',\n",
       " 'cp-0015.ckpt.index',\n",
       " 'cp-0010.ckpt.index',\n",
       " 'cp-0045.ckpt.index',\n",
       " 'cp-0040.ckpt.data-00000-of-00001',\n",
       " 'cp-0050.ckpt.index',\n",
       " 'cp-0030.ckpt.index',\n",
       " 'cp-0000.ckpt.index',\n",
       " 'cp-0050.ckpt.data-00000-of-00001',\n",
       " 'cp-0035.ckpt.index',\n",
       " 'cp-0005.ckpt.index',\n",
       " 'cp-0020.ckpt.index',\n",
       " 'cp-0015.ckpt.data-00000-of-00001',\n",
       " 'cp-0045.ckpt.data-00000-of-00001',\n",
       " 'cp-0000.ckpt.data-00000-of-00001',\n",
       " 'checkpoint',\n",
       " 'cp-0020.ckpt.data-00000-of-00001',\n",
       " 'cp-0005.ckpt.data-00000-of-00001',\n",
       " 'cp-0025.ckpt.data-00000-of-00001',\n",
       " 'cp-0035.ckpt.data-00000-of-00001']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(checkpoint_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee98eb2",
   "metadata": {},
   "source": [
    "choose the latest one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41ed6a00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'training_2/cp-0050.ckpt'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "latest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a4877b",
   "metadata": {},
   "source": [
    "To test, reset the model, and load the latest checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8a400bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 649us/step - loss: 0.5048 - sparse_categorical_accuracy: 0.8750\n",
      "Restored model, accuracy: 87.50%\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "\n",
    "model.load_weights(latest)\n",
    "\n",
    "loss, acc = model.evaluate(test_images, test_labels, verbose=1)\n",
    "print('Restored model, accuracy: {:5.2f}%'.format(100*acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406c6cf0",
   "metadata": {},
   "source": [
    "## The files\n",
    "\n",
    "The above code stores the weights to a collection of checkpoint-formatted files that contain only the trained weights in a binary format. Checkpoints contain:\n",
    "- One or more shards that contain your model's weights.\n",
    "- An index file that indicates which weights are stored in which shard."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc89ee85",
   "metadata": {},
   "source": [
    "## Manually save weights\n",
    "\n",
    "To save weights manually, use `tf.keras.Model.save_weights`. By default, `tf.keras`—and the `Model.save_weights` method in particular—uses the **TensorFlow Checkpoint** format with a `.ckpt` extension. To save in the **HDF5** format with a `.h5` extension, refer to the Save and load models guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2b91be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 647us/step - loss: 0.5048 - sparse_categorical_accuracy: 0.8750\n",
      "Restored model, accuracy: 87.50%\n"
     ]
    }
   ],
   "source": [
    "model.save_weights('./checkpoints/my_checkpoint')\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "#Restore\n",
    "model.load_weights('./checkpoints/my_checkpoint')\n",
    "\n",
    "loss, acc = model.evaluate(test_images, test_labels, verbose=1)\n",
    "print('Restored model, accuracy: {:5.2f}%'.format(100*acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbc0980",
   "metadata": {},
   "source": [
    "## Save the entire model\n",
    "`Call tf.keras.Model.save` to save a model's architecture, weights, and training configuration in a single file/folder. This allows to export a model so it can be used without access to the original Python code*. Since the optimizer-state is recovered, we can resume training from exactly where we left off.\n",
    "<br><br>\n",
    "An entire model can be saved in two different file formats (`SavedModel` and `HDF5`). The TensorFlow `SavedModel` format is the default file format in TF2.x. However, models can be saved in `HDF5` format. More details on saving entire models in the two file formats is described below.\n",
    "<br><br>\n",
    "Saving a fully-functional model is very useful—we can load them in TensorFlow.js (Saved Model, HDF5) and then train and run them in web browsers, or convert them to run on mobile devices using TensorFlow Lite (Saved Model, HDF5)\n",
    "#### *Custom objects (for example, subclassed models or layers) require special attention when saving and loading. Refer to the Saving custom objects section below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8d1525",
   "metadata": {},
   "source": [
    "### SavedModel format\n",
    "\n",
    "The SavedModel format is another way to serialize models. Models saved in this format can be restored using `tf.keras.models.load_model` and are compatible with TensorFlow Serving. The SavedModel guide goes into detail about how to `serve/inspect` the SavedModel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "052f2e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1248 - sparse_categorical_accuracy: 0.6860\n",
      "Epoch 2/5\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4147 - sparse_categorical_accuracy: 0.8800\n",
      "Epoch 3/5\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2923 - sparse_categorical_accuracy: 0.9200\n",
      "Epoch 4/5\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2054 - sparse_categorical_accuracy: 0.9520\n",
      "Epoch 5/5\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1490 - sparse_categorical_accuracy: 0.9700\n",
      "INFO:tensorflow:Assets written to: saved_model/my_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-29 09:05:13.008584: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "model.fit(train_images, train_labels, epochs=5)\n",
    "\n",
    "#Save the entire model as a SavedModel\n",
    "!mkdir -p saved_model\n",
    "model.save('saved_model/my_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8847107",
   "metadata": {},
   "source": [
    "The SavedModel format is a directory containing a protobuf binary and a TensorFlow checkpoint. Inspect the saved model directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd880d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mmy_model\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls saved_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1353eedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34massets\u001b[0m/  keras_metadata.pb  saved_model.pb  \u001b[01;34mvariables\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls saved_model/my_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eef6e9c",
   "metadata": {},
   "source": [
    "### Reload a fresh keras model from the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6150675a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_12 (Dense)            (None, 512)               401920    \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_model = tf.keras.models.load_model('saved_model/my_model')\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3f017bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 661us/step - loss: 0.4122 - sparse_categorical_accuracy: 0.8630\n",
      "Restored model, accuracy: 86.30%\n",
      "(1000, 10)\n"
     ]
    }
   ],
   "source": [
    "loss, acc = new_model.evaluate(test_images,test_labels, verbose=1)\n",
    "print('Restored model, accuracy: {:5.2f}%'.format(100*acc))\n",
    "\n",
    "print(new_model.predict(test_images).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba31b7d9",
   "metadata": {},
   "source": [
    "## HDF5 format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "43a6b403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n",
      "Epoch 1/5\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1564 - sparse_categorical_accuracy: 0.6630\n",
      "Epoch 2/5\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4191 - sparse_categorical_accuracy: 0.8830\n",
      "Epoch 3/5\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2818 - sparse_categorical_accuracy: 0.9250\n",
      "Epoch 4/5\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2093 - sparse_categorical_accuracy: 0.9440\n",
      "Epoch 5/5\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1481 - sparse_categorical_accuracy: 0.9700\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "model.fit(train_images, train_labels, epochs=5)\n",
    "\n",
    "model.save('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5e0f5fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_18 (Dense)            (None, 512)               401920    \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_model = tf.keras.models.load_model('my_model.h5')\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "60a78287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 623us/step - loss: 0.4164 - sparse_categorical_accuracy: 0.8630\n",
      "Restored model, accuracy: 86.30%\n"
     ]
    }
   ],
   "source": [
    "loss, acc = new_model.evaluate(test_images, test_labels, verbose=1)\n",
    "print('Restored model, accuracy: {:5.2f}%'.format(100 * acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe2f6b2",
   "metadata": {},
   "source": [
    "Keras saves models by inspecting their architectures. This technique saves everything:\n",
    "- The weight values\n",
    "- The model's training configuration (what is passed to the `.compile()` method)\n",
    "- The optimizer and its state, if any (this enables to restart training where we left off)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4352eed9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
